{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXucheowMg-Z"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from collections import deque\n",
        "import random\n",
        "\n",
        "import networkx as nx\n",
        "# import igraph as ig\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, roc_auc_score, precision_recall_curve\n",
        "\n",
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hf_df = pd.read_csv('C:/Users/DATOP/data/dataset_002/HF_TRNS_TRAM.csv', low_memory=False)\n",
        "\n",
        "print(\"데이터 로드 완료\")\n",
        "print(hf_df.head())"
      ],
      "metadata": {
        "id": "cKhgpCeDNFus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 'ff_sp_ai' 컬럼 값이 '01', '02', 'SP'인 데이터만 선택\n",
        "hf_df_filtered_1 = hf_df[(hf_df['ff_sp_ai'] == '01') | (hf_df['ff_sp_ai'] == '02') | (hf_df['ff_sp_ai'] == 'SP')]\n",
        "\n",
        "# 'ff_sp_ai' 컬럼 값이 '01', '02', 'SP'가 아닌 데이터만 선택\n",
        "hf_df_filtered_2 = hf_df[~((hf_df['ff_sp_ai'] == '01') | (hf_df['ff_sp_ai'] == '02') | (hf_df['ff_sp_ai'] == 'SP'))]\n",
        "\n",
        "print(hf_df_filtered_1.shape)\n",
        "print(hf_df_filtered_2.shape)\n",
        "# (9321, 10)\n",
        "# (10542643, 10)\n",
        "\n",
        "# 위에서 필터링한 두 번째 데이터프레임(hf_df_filtered_2)에서 1%의 데이터를 무작위로 샘플링\n",
        "hf_df_filtered_2_sampled = hf_df_filtered_2.sample(frac=0.01, random_state=42)\n",
        "hf_df_filtered_2_sampled.shape\n",
        "# (105426, 10)\n",
        "\n",
        "# 첫 번째 데이터프레임과, 두 번째 데이터프레임에서 샘플링한 결과를 하나로 합치기\n",
        "hf_sample_df = pd.concat([hf_df_filtered_1, hf_df_filtered_2_sampled], ignore_index=True)\n",
        "\n",
        "hf_sample_df.shape\n",
        "# (114747, 10)\n",
        "\n",
        "hf_sample_df.head()"
      ],
      "metadata": {
        "id": "d38M7l_fNWeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# datetime 컬럼 생성\n",
        "hf_sample_df['datetime'] = pd.to_datetime(hf_sample_df['tran_dt'], format='%Y%m%d') + hf_sample_df['tran_tmrg'].astype('timedelta64[h]')\n",
        "\n",
        "# wd_account 컬럼 생성 (wd_fc_sn + wd_ac_sn을 문자열로 결합)\n",
        "hf_sample_df['wd_account'] = hf_sample_df['wd_fc_sn'].astype(str) + hf_sample_df['wd_ac_sn'].astype(str)\n",
        "\n",
        "# dps_account 컬럼 생성 (dps_fc_sn + dps_ac_sn을 문자열로 결합)\n",
        "hf_sample_df['dps_account'] = hf_sample_df['dps_fc_sn'].astype(str) + hf_sample_df['dps_ac_sn'].astype(str)"
      ],
      "metadata": {
        "id": "wnQMJk6fN_N5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 기초 통계량 피쳐"
      ],
      "metadata": {
        "id": "cJkx1dHeOP3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 기초 통계량 피처를 만들기 위해 데이터프레임을 복사\n",
        "hf_basic_stats_df = hf_sample_df\n",
        "\n",
        "# 출금 계좌별로 정렬\n",
        "hf_basic_stats_df = hf_basic_stats_df.sort_values(['wd_account', 'datetime']).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "u7c_FwoGOWXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sliding_window_count(group, window):\n",
        "\n",
        "    times = group['datetime'].values\n",
        "    n = len(times)\n",
        "\n",
        "    counts = np.zeros(n, dtype=int)\n",
        "    start = 0\n",
        "\n",
        "    for i in range(n):\n",
        "        t = times[i]\n",
        "\n",
        "        while t - times[start] > np.timedelta64(window):\n",
        "            start += 1\n",
        "\n",
        "        # counts[i] = i - start + 1\n",
        "        counts[i] = i - start\n",
        "\n",
        "    return pd.Series(counts, index=group.index)"
      ],
      "metadata": {
        "id": "79ZLN66DOq3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import deque\n",
        "\n",
        "def sliding_window_total(group, window):\n",
        "    q = deque()\n",
        "    total = 0\n",
        "    result = []\n",
        "\n",
        "    for dt, amt in zip(group['datetime'], group['tran_amt']):\n",
        "        # 새 거래 추가\n",
        "        q.append((dt, amt))\n",
        "        total += amt\n",
        "\n",
        "        # window 벗어난 거래 제거\n",
        "        while q and (dt - q[0][0]) > window:\n",
        "            old_dt, old_amt = q.popleft()\n",
        "            total -= old_amt\n",
        "\n",
        "        result.append(total)\n",
        "\n",
        "    return pd.Series(result, index=group.index)"
      ],
      "metadata": {
        "id": "dc_1CuXkPI8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 피처: 최근 시간별, 일별 송금횟수 피처 생성\n",
        "hf_basic_stats_df['송금횟수_최근1시간'] = (\n",
        "    hf_basic_stats_df.groupby('wd_account', group_keys=False).apply(sliding_window_count, window=pd.Timedelta(hours=1))\n",
        ")\n",
        "\n",
        "hf_basic_stats_df['송금횟수_최근3시간'] = (\n",
        "    hf_basic_stats_df.groupby('wd_account', group_keys=False).apply(sliding_window_count, window=pd.Timedelta(hours=3))\n",
        ")\n",
        "\n",
        "hf_basic_stats_df['송금횟수_최근12시간'] = (\n",
        "    hf_basic_stats_df.groupby('wd_account', group_keys=False).apply(sliding_window_count, window=pd.Timedelta(hours=12))\n",
        ")\n",
        "\n",
        "hf_basic_stats_df['송금횟수_최근1일'] = (\n",
        "    hf_basic_stats_df.groupby('wd_account', group_keys=False).apply(sliding_window_count, window=pd.Timedelta(days=1))\n",
        ")\n",
        "\n",
        "hf_basic_stats_df['송금횟수_최근7일'] = (\n",
        "    hf_basic_stats_df.groupby('wd_account', group_keys=False).apply(sliding_window_count, window=pd.Timedelta(days=7))\n",
        ")\n",
        "\n",
        "hf_basic_stats_df['송금횟수_최근30일'] = (\n",
        "    hf_basic_stats_df.groupby('wd_account', group_keys=False).apply(sliding_window_count, window=pd.Timedelta(days=30))\n",
        ")\n",
        "\n",
        "hf_basic_stats_df['송금횟수_최근90일'] = (\n",
        "    hf_basic_stats_df.groupby('wd_account', group_keys=False).apply(sliding_window_count, window=pd.Timedelta(days=90))\n",
        ")"
      ],
      "metadata": {
        "id": "kbP5A6V2PfOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. 피처: 최근 시간별, 일별 송금총액 피처 생성\n",
        "hf_basic_stats_df['송금총액_최근1시간'] = (\n",
        "    hf_basic_stats_df.groupby('wd_account', group_keys=False)\n",
        "    .apply(lambda x: sliding_window_total(x, window=pd.Timedelta(hours=1)))\n",
        ")\n",
        "\n",
        "hf_basic_stats_df['송금총액_최근3시간'] = (\n",
        "    hf_basic_stats_df.groupby('wd_account', group_keys=False)\n",
        "    .apply(lambda x: sliding_window_total(x, window=pd.Timedelta(hours=3)))\n",
        ")\n",
        "\n",
        "hf_basic_stats_df['송금총액_최근12시간'] = (\n",
        "    hf_basic_stats_df.groupby('wd_account', group_keys=False)\n",
        "    .apply(lambda x: sliding_window_total(x, window=pd.Timedelta(hours=12)))\n",
        ")\n",
        "\n",
        "hf_basic_stats_df['송금총액_최근1일'] = (\n",
        "    hf_basic_stats_df.groupby('wd_account', group_keys=False)\n",
        "    .apply(lambda x: sliding_window_total(x, window=pd.Timedelta(days=1)))\n",
        ")\n",
        "\n",
        "hf_basic_stats_df['송금총액_최근7일'] = (\n",
        "    hf_basic_stats_df.groupby('wd_account', group_keys=False)\n",
        "    .apply(lambda x: sliding_window_total(x, window=pd.Timedelta(days=7)))\n",
        ")\n",
        "\n",
        "hf_basic_stats_df['송금총액_최근30일'] = (\n",
        "    hf_basic_stats_df.groupby('wd_account', group_keys=False)\n",
        "    .apply(lambda x: sliding_window_total(x, window=pd.Timedelta(days=30)))\n",
        ")\n",
        "\n",
        "hf_basic_stats_df['송금총액_최근90일'] = (\n",
        "    hf_basic_stats_df.groupby('wd_account', group_keys=False)\n",
        "    .apply(lambda x: sliding_window_total(x, window=pd.Timedelta(days=90)))\n",
        ")"
      ],
      "metadata": {
        "id": "cu6sIq-cPvZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_basic_stats_df = hf_basic_stats_df.sort_values(['dps_account', 'datetime']).reset_index(drop=True)\n",
        "\n",
        "# 3. 피처: 최근 시간별, 일별 입금횟수 피처 생성\n",
        "hf_basic_stats_df['입금횟수_최근1시간'] = (\n",
        "    hf_basic_stats_df.groupby('dps_account', group_keys=False).apply(sliding_window_count, window=pd.Timedelta(hours=1))\n",
        ")\n",
        "\n",
        "hf_basic_stats_df['입금횟수_최근3시간'] = (\n",
        "    hf_basic_stats_df.groupby('dps_account', group_keys=False).apply(sliding_window_count, window=pd.Timedelta(hours=3))\n",
        ")\n",
        "\n",
        "hf_basic_stats_df['입금횟수_최근12시간'] = (\n",
        "    hf_basic_stats_df.groupby('dps_account', group_keys=False).apply(sliding_window_count, window=pd.Timedelta(hours=12))\n",
        ")\n",
        "\n",
        "hf_basic_stats_df['입금횟수_최근1일'] = (\n",
        "    hf_basic_stats_df.groupby('dps_account', group_keys=False).apply(sliding_window_count, window=pd.Timedelta(days=1))\n",
        ")\n",
        "\n",
        "hf_basic_stats_df['입금횟수_최근7일'] = (\n",
        "    hf_basic_stats_df.groupby('dps_account', group_keys=False).apply(sliding_window_count, window=pd.Timedelta(days=7))\n",
        ")\n",
        "\n",
        "hf_basic_stats_df['입금횟수_최근30일'] = (\n",
        "    hf_basic_stats_df.groupby('dps_account', group_keys=False).apply(sliding_window_count, window=pd.Timedelta(days=30))\n",
        ")\n",
        "\n",
        "hf_basic_stats_df['입금횟수_최근90일'] = (\n",
        "    hf_basic_stats_df.groupby('dps_account', group_keys=False).apply(sliding_window_count, window=pd.Timedelta(days=90))\n",
        ")"
      ],
      "metadata": {
        "id": "u4UyCbenQJ6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. 피처: 최근 시간별, 일별 입금총액 피처 생성\n",
        "hf_basic_stats_df['입금총액_최근1시간'] = (\n",
        "    hf_basic_stats_df.groupby('dps_account', group_keys=False)\n",
        "    .apply(lambda x: sliding_window_total(x, window=pd.Timedelta(hours=1)))\n",
        ")\n",
        "\n",
        "hf_basic_stats_df['입금총액_최근3시간'] = (\n",
        "    hf_basic_stats_df.groupby('dps_account', group_keys=False)\n",
        "    .apply(lambda x: sliding_window_total(x, window=pd.Timedelta(hours=3)))\n",
        ")\n",
        "\n",
        "hf_basic_stats_df['입금총액_최근12시간'] = (\n",
        "    hf_basic_stats_df.groupby('dps_account', group_keys=False)\n",
        "    .apply(lambda x: sliding_window_total(x, window=pd.Timedelta(hours=12)))\n",
        ")\n",
        "\n",
        "hf_basic_stats_df['입금총액_최근1일'] = (\n",
        "    hf_basic_stats_df.groupby('dps_account', group_keys=False)\n",
        "    .apply(lambda x: sliding_window_total(x, window=pd.Timedelta(days=1)))\n",
        ")\n",
        "\n",
        "hf_basic_stats_df['입금총액_최근7일'] = (\n",
        "    hf_basic_stats_df.groupby('dps_account', group_keys=False)\n",
        "    .apply(lambda x: sliding_window_total(x, window=pd.Timedelta(days=7)))\n",
        ")\n",
        "\n",
        "hf_basic_stats_df['입금총액_최근30일'] = (\n",
        "    hf_basic_stats_df.groupby('dps_account', group_keys=False)\n",
        "    .apply(lambda x: sliding_window_total(x, window=pd.Timedelta(days=30)))\n",
        ")\n",
        "\n",
        "hf_basic_stats_df['입금총액_최근90일'] = (\n",
        "    hf_basic_stats_df.groupby('dps_account', group_keys=False)\n",
        "    .apply(lambda x: sliding_window_total(x, window=pd.Timedelta(days=90)))\n",
        ")"
      ],
      "metadata": {
        "id": "ViaBn9tCQrhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. 피처: 거래 상대방 계좌와의 첫 거래 여부\n",
        "# txn_rank: 몇번째 거래인가\n",
        "# is_first_txn: 첫번째 거래 여부\n",
        "hf_basic_stats_df['txn_rank'] = hf_basic_stats_df.groupby('wd_account')['datetime'].rank(method='first')\n",
        "hf_basic_stats_df['is_first_txn'] = (hf_basic_stats_df['txn_rank'] == 1).astype(int)\n",
        "\n",
        "hf_basic_stats_df.drop(['txn_rank'], axis=1, inplace=True)\n",
        "\n",
        "# 6. 피처: 전체 거래량 대비 송금 거래량 비율\n",
        "hf_basic_stats_df['송금거래량비율_최근30일'] = hf_basic_stats_df['송금횟수_최근30일'] / (hf_basic_stats_df['송금횟수_최근30일'] + hf_basic_stats_df['입금횟수_최근30일']+ 1e-10)\n",
        "hf_basic_stats_df['송금거래량비율_최근90일'] = hf_basic_stats_df['송금횟수_최근90일'] / (hf_basic_stats_df['송금횟수_최근90일'] + hf_basic_stats_df['입금횟수_최근90일']+ 1e-10)\n",
        "hf_basic_stats_df['입금거래량비율_최근30일'] = hf_basic_stats_df['입금횟수_최근30일'] / (hf_basic_stats_df['송금횟수_최근30일'] + hf_basic_stats_df['입금횟수_최근30일']+ 1e-10)\n",
        "hf_basic_stats_df['입금거래량비율_최근90일'] = hf_basic_stats_df['입금횟수_최근90일'] / (hf_basic_stats_df['송금횟수_최근90일'] + hf_basic_stats_df['입금횟수_최근90일']+ 1e-10)\n",
        "\n",
        "# 0으로 나누는 경우 Null -> 0으로 대체\n",
        "hf_basic_stats_df['송금거래량비율_최근30일'] = hf_basic_stats_df['송금거래량비율_최근30일'].fillna(0)\n",
        "hf_basic_stats_df['송금거래량비율_최근90일'] = hf_basic_stats_df['송금거래량비율_최근90일'].fillna(0)\n",
        "hf_basic_stats_df['입금거래량비율_최근30일'] = hf_basic_stats_df['입금거래량비율_최근30일'].fillna(0)\n",
        "hf_basic_stats_df['입금거래량비율_최근90일'] = hf_basic_stats_df['입금거래량비율_최근90일'].fillna(0)"
      ],
      "metadata": {
        "id": "tK_P7RLaQyya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. 피처: 매체별 송금/입금 횟수\n",
        "media_codes = list(sorted(hf_basic_stats_df['md_type'].unique())) # [1,2,3,4,5,6,7]\n",
        "\n",
        "for md in media_codes:\n",
        "    # 출금계좌별로 정렬\n",
        "    hf_basic_stats_df = hf_basic_stats_df.sort_values(['wd_account', 'datetime']).reset_index(drop=True)\n",
        "    # 매체별 송금횟수 집계\n",
        "    hf_basic_stats_df[f'송금횟수_매체구분{md}_최근3시간'] = (\n",
        "        hf_basic_stats_df[hf_basic_stats_df['md_type'] == md]\n",
        "        .groupby('wd_account', group_keys=False)\n",
        "        .apply(sliding_window_count, window=pd.Timedelta(hours=3))\n",
        "    ).astype(int)\n",
        "    hf_basic_stats_df[f'송금횟수_매체구분{md}_최근12시간'] = (\n",
        "        hf_basic_stats_df[hf_basic_stats_df['md_type'] == md]\n",
        "        .groupby('wd_account', group_keys=False)\n",
        "        .apply(sliding_window_count, window=pd.Timedelta(hours=12))\n",
        "    ).astype(int)\n",
        "\n",
        "    # 입금계좌별로 정렬\n",
        "    hf_basic_stats_df = hf_basic_stats_df.sort_values(['dps_account', 'datetime']).reset_index(drop=True)\n",
        "    # 매체별 입금횟수 집계\n",
        "    hf_basic_stats_df[f'입금횟수_매체구분{md}_최근3시간'] = (\n",
        "        hf_basic_stats_df[hf_basic_stats_df['md_type'] == md]\n",
        "        .groupby('dps_account', group_keys=False)\n",
        "        .apply(sliding_window_count, window=pd.Timedelta(hours=3))\n",
        "    ).astype(int)\n",
        "    hf_basic_stats_df[f'입금횟수_매체구분{md}_최근12시간'] = (\n",
        "        hf_basic_stats_df[hf_basic_stats_df['md_type'] == md]\n",
        "        .groupby('dps_account', group_keys=False)\n",
        "        .apply(sliding_window_count, window=pd.Timedelta(hours=12))\n",
        "    ).astype(int)\n",
        "\n",
        "# 결측치 처리: 송금(입금) 횟수 없음 Null -> 0 으로 대체\n",
        "hf_basic_stats_df[f'송금횟수_매체구분{md}_최근3시간'] = hf_basic_stats_df[f'송금횟수_매체구분{md}_최근3시간'].fillna(0).astype(int)\n",
        "hf_basic_stats_df[f'송금횟수_매체구분{md}_최근12시간'] = hf_basic_stats_df[f'송금횟수_매체구분{md}_최근12시간'].fillna(0).astype(int)\n",
        "hf_basic_stats_df[f'입금횟수_매체구분{md}_최근3시간'] = hf_basic_stats_df[f'입금횟수_매체구분{md}_최근3시간'].fillna(0).astype(int)\n",
        "hf_basic_stats_df[f'입금횟수_매체구분{md}_최근12시간'] = hf_basic_stats_df[f'입금횟수_매체구분{md}_최근12시간'].fillna(0).astype(int)\n"
      ],
      "metadata": {
        "id": "iEhN9tR_RdYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 불필요한 컬럼 제거\n",
        "hf_basic_stats_df.drop(columns=['tran_dt', 'tran_tmrg', 'wd_fc_sn', 'wd_ac_sn', 'dps_fc_sn', 'dps_ac_sn'], axis=1, inplace=True)\n",
        "\n",
        "# 최종 컬럼 확인\n",
        "hf_basic_stats_df.columns"
      ],
      "metadata": {
        "id": "u0efw7GeRvG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 최종적으로 사용할 컬럼들만 선택 및 순서 재배치\n",
        "hf_basic_stats_df = hf_basic_stats_df[['datetime', 'wd_account',\n",
        "    'dps_account', 'tran_amt', 'md_type', 'fnd_type', '송금횟수_최근1시간', '송금횟수_최근3시간', '송금횟수_최근12시간', '송금횟수_최근1일', '송금횟수_최근7일',\n",
        "    '송금횟수_최근30일', '송금횟수_최근90일', '송금총액_최근1시간', '송금총액_최근3시간', '송금총액_최근12시간', '송금총액_최근1일', '송금총액_최근7일', '송금총액_최근30일', '송금총액_최근90일',\n",
        "    '입금횟수_최근1시간', '입금횟수_최근3시간', '입금횟수_최근12시간', '입금횟수_최근1일', '입금횟수_최근7일', '입금횟수_최근30일', '입금횟수_최근90일',\n",
        "    '입금총액_최근1시간', '입금총액_최근3시간', '입금총액_최근12시간', '입금총액_최근1일', '입금총액_최근7일', '입금총액_최근30일', '입금총액_최근90일', 'is_first_txn',\n",
        "    '송금거래량비율_최근30일', '송금거래량비율_최근90일', '입금거래량비율_최근30일', '입금거래량비율_최근90일',\n",
        "    '송금횟수_매체구분1_최근3시간', '송금횟수_매체구분1_최근12시간', '입금횟수_매체구분1_최근3시간', '입금횟수_매체구분1_최근12시간',\n",
        "    '송금횟수_매체구분2_최근3시간', '송금횟수_매체구분2_최근12시간', '입금횟수_매체구분2_최근3시간', '입금횟수_매체구분2_최근12시간',\n",
        "    '송금횟수_매체구분3_최근3시간', '송금횟수_매체구분3_최근12시간', '입금횟수_매체구분3_최근3시간', '입금횟수_매체구분3_최근12시간',\n",
        "    '송금횟수_매체구분4_최근3시간', '송금횟수_매체구분4_최근12시간', '입금횟수_매체구분4_최근3시간', '입금횟수_매체구분4_최근12시간',\n",
        "    '송금횟수_매체구분5_최근3시간', '송금횟수_매체구분5_최근12시간', '입금횟수_매체구분5_최근3시간', '입금횟수_매체구분5_최근12시간',\n",
        "    '송금횟수_매체구분6_최근3시간', '송금횟수_매체구분6_최근12시간', '입금횟수_매체구분6_최근3시간', '입금횟수_매체구분6_최근12시간',\n",
        "    '송금횟수_매체구분7_최근3시간', '송금횟수_매체구분7_최근12시간', '입금횟수_매체구분7_최근3시간', '입금횟수_매체구분7_최근12시간',\n",
        "    'ff_sp_ai']]\n",
        "\n",
        "# 데이터프레임의 요약 정보 출력\n",
        "hf_basic_stats_df.info()"
      ],
      "metadata": {
        "id": "jsRB8aaNSZIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_basic_stats_df['wd_account'] = hf_basic_stats_df['wd_account'].astype('int64')\n",
        "hf_basic_stats_df['dps_account'] = hf_basic_stats_df['dps_account'].astype('int64')\n",
        "\n",
        "hf_basic_stats_df"
      ],
      "metadata": {
        "id": "Jv2NyPE-SsxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 그래프 피쳐"
      ],
      "metadata": {
        "id": "-n7qwGwPTdJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 그래프 피처 생성을 위한 별도의 데이터프레임 준비\n",
        "hf_graph_df = hf_sample_df\n",
        "\n",
        "# 출금 계좌별로 정렬\n",
        "hf_graph_df = hf_graph_df.sort_values(['wd_account', 'datetime']).reset_index(drop=True)\n",
        "\n",
        "# 불필요한 원본 컬럼 제거\n",
        "hf_graph_df.drop(columns=['tran_dt', 'tran_tmrg', 'wd_fc_sn', 'wd_ac_sn', 'dps_fc_sn', 'dps_ac_sn'], axis=1, inplace=True)\n",
        "\n",
        "# 그래프 생성에 필요한 컬럼만 선택 및 재정렬\n",
        "hf_graph_df = hf_graph_df[['datetime', 'wd_account', 'dps_account', 'tran_amt', 'md_type', 'fnd_type', 'ff_sp_ai']]"
      ],
      "metadata": {
        "id": "G3PzkJ0NTgY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## MultiDiGraph 구축 함수\n",
        "def build_multidigraph(df):\n",
        "    G = nx.MultiDiGraph()\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        from_acc = row['wd_account']\n",
        "        to_acc = row['dps_account']\n",
        "        attrs = {\n",
        "            'datetime': row['datetime'],\n",
        "            'amount': row['tran_amt'],\n",
        "            'channel': row['md_type'],\n",
        "            'fund_type': row['fnd_type'],\n",
        "            'fraud_flag': row['ff_sp_ai']\n",
        "        }\n",
        "\n",
        "        G.add_edge(from_acc, to_acc, **attrs)\n",
        "\n",
        "    return G\n",
        "\n",
        "G = build_multidigraph(hf_graph_df)"
      ],
      "metadata": {
        "id": "Zos8PMhkUIJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========= 1. 중심성 ======== #\n",
        "centrality_df = pd.DataFrame({\n",
        "    'node': list(G.nodes()),\n",
        "    'degree_centrality': pd.Series(nx.degree_centrality(G)),\n",
        "    'betweenness': pd.Series(nx.betweenness_centrality(G)),\n",
        "    'closeness': pd.Series(nx.closeness_centrality(G)),\n",
        "    'eigenvector': pd.Series(nx.eigenvector_centrality_numpy(G)),\n",
        "    'pagerank': pd.Series(nx.pagerank(G))\n",
        "}).reset_index(drop=True)\n",
        "\n",
        "centrality_df"
      ],
      "metadata": {
        "id": "DH3MjOrqUKh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======== 2. 차수 ======== #\n",
        "degree_df = pd.DataFrame({\n",
        "    'node': list(G.nodes()),\n",
        "    'in_degree': pd.Series(dict(G.in_degree())),\n",
        "    'out_degree': pd.Series(dict(G.out_degree()))\n",
        "}).reset_index(drop=True)\n",
        "\n",
        "degree_df[\"degree\"] = degree_df[\"in_degree\"] + degree_df[\"out_degree\"]\n",
        "\n",
        "degree_df"
      ],
      "metadata": {
        "id": "jeFCvfQTUynW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======== 3. 거래 파트너 다양성: 출금 계좌에 연결된 입금 계좌수 ======== #\n",
        "partner_div_df = pd.DataFrame({\n",
        "    \"node\": list(G.nodes()),\n",
        "    \"partner_div\": [\n",
        "        len(set(G.predecessors(n)).union(set(G.successors(n))))\n",
        "        for n in G.nodes()\n",
        "    ]\n",
        "})\n",
        "\n",
        "partner_div_df"
      ],
      "metadata": {
        "id": "fhXQ51ynVJuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======== 4. Louvain 커뮤니티 탐지======== #\n",
        "# import community as community_louvain\n",
        "# partition = community_louvain.best_partition(G.to_undirected())\n",
        "# comm_df = pd.DataFrame(list(partition.items()), columns=[\"계좌\", \"community_id\"])\n",
        "# comm_size = comm_df[\"community_id\"].value_counts().to_dict()\n",
        "# comm_df[\"community_size\"] = comm_df[\"community_id\"].map(comm_size).astype(int)\n",
        "# comm_df.set_index(\"계좌\", inplace=True)\n",
        "\n",
        "\n",
        "# ======== 5. 왕복 거래 여부 피처======== #\n",
        "round_trip_df = pd.DataFrame({\n",
        "    \"node\": list(G.nodes()),\n",
        "    \"round_trip\": [\n",
        "        any(G.has_edge(n, nbr) and G.has_edge(nbr, n) for nbr in G.successors(n))\n",
        "        for n in G.nodes()\n",
        "    ]\n",
        "})\n",
        "\n",
        "round_trip_df"
      ],
      "metadata": {
        "id": "GO3c-EmOVZiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======== 6. 의심계좌 근접도======== #\n",
        "suspicious_nodes = [n for u, v, d in G.edges(data=True) if d.get(\"fraud_flag\") == 1 for n in [u, v]]\n",
        "\n",
        "suspect_proximity_df = pd.DataFrame({\n",
        "    \"node\": list(G.nodes()),\n",
        "    \"suspect_proximity\": [\n",
        "        min([\n",
        "            nx.shortest_path_length(G, source=n, target=s)\n",
        "            for s in suspicious_nodes\n",
        "            if nx.has_path(G, n, s)\n",
        "        ]) if suspicious_nodes else None\n",
        "        for n in G.nodes()\n",
        "    ]\n",
        "})\n",
        "\n",
        "suspect_proximity_df"
      ],
      "metadata": {
        "id": "Ca7ryqHoVpIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======== 7. Gather-scatter (한 계좌에서 여러 계좌로 분산 송금) ========\n",
        "in_degree = dict(G.in_degree())\n",
        "out_degree = dict(G.out_degree())\n",
        "\n",
        "gather_scatter_df = pd.DataFrame({\n",
        "    \"node\": list(G.nodes()),\n",
        "    \"gather_scatter\": [in_degree.get(n, 0) - out_degree.get(n, 0) for n in G.nodes()]\n",
        "})\n",
        "\n",
        "# ======== 8. Scatter-gather (여러 계좌에서 한 계좌로 집중 송금) ========\n",
        "scatter_gather_df = pd.DataFrame({\n",
        "    \"node\": list(G.nodes()),\n",
        "    \"scatter_gather\": [out_degree.get(n, 0) - in_degree.get(n, 0) for n in G.nodes()]\n",
        "})\n",
        "\n",
        "# ======== 9. Bipartite 변환 (출금 계좌 vs 입금 계좌) ========\n",
        "# 출금=1, 입금=0 구분\n",
        "bipartite_set1 = set(hf_graph_df[\"wd_account\"])\n",
        "bipartite_df = pd.DataFrame({\n",
        "    \"node\": list(G.nodes()),\n",
        "    \"bipartite\": [1 if n in bipartite_set1 else 0 for n in G.nodes()]\n",
        "})"
      ],
      "metadata": {
        "id": "ujbLTAOwWE76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======== 10. Simple cycle 탐지 ======== #\n",
        "cycles = list(nx.simple_cycles(G))\n",
        "simple_cycle_df = pd.DataFrame({\n",
        "    \"node\": list(G.nodes()),\n",
        "    \"simple_cycle\": [any(n in cycle for cycle in cycles) for n in G.nodes()]\n",
        "})\n",
        "\n",
        "# ======== 11. stack (여러 계좌로 송금) ======== #\n",
        "stack_df = pd.DataFrame({\n",
        "    \"node\": list(G.nodes()),\n",
        "    \"stack\": [1 if len(set(G.successors(n))) > 1 else 0 for n in G.nodes()]\n",
        "})"
      ],
      "metadata": {
        "id": "lFB2ZGKqWR4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======== 12. random walk ======== #\n",
        "def random_walk_probability(G, start, steps=100, walk_length=10):\n",
        "    visits = {n: 0 for n in G.nodes()}\n",
        "    for _ in range(steps):\n",
        "        current = start\n",
        "        for _ in range(walk_length):\n",
        "            neighbors = list(G.successors(current))\n",
        "            if not neighbors:\n",
        "                break\n",
        "            current = random.choice(neighbors)\n",
        "            visits[current] += 1\n",
        "\n",
        "    total = sum(visits.values())\n",
        "    return {n: visits[n] / total if total > 0 else 0 for n in G.nodes()}\n",
        "\n",
        "random_walk_feat = random_walk_probability(G, start=random.choice(list(G.nodes())))\n",
        "random_walk_df = pd.DataFrame({\n",
        "    \"node\": list(G.nodes()),\n",
        "    \"random_walk\": pd.Series(random_walk_feat)\n",
        "}).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "r1VMcsD1WfkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모든 피처 병합\n",
        "graph_features = (centrality_df\n",
        "    .merge(degree_df, on='node', how='outer')\n",
        "    .merge(partner_div_df, on='node', how='outer')\n",
        "    .merge(round_trip_df, on='node', how='outer')\n",
        "    .merge(suspect_proximity_df, on='node', how='outer')\n",
        "    .merge(gather_scatter_df, on='node', how='outer')\n",
        "    .merge(scatter_gather_df, on='node', how='outer')\n",
        "    .merge(bipartite_df, on='node', how='outer')\n",
        "    .merge(simple_cycle_df, on='node', how='outer')\n",
        "    .merge(stack_df, on='node', how='outer')\n",
        "    .merge(random_walk_df)).fillna(0)\n",
        "\n",
        "print(graph_features.shape)\n",
        "print(hf_graph_df.shape)"
      ],
      "metadata": {
        "id": "7KQBeUM6WwcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## node 기반 피처 -> 거래 단위 dataframe으로 확장하려면\n",
        "# - 출금/입금 계좌 기준으로 각각 병합\n",
        "# - prefix(src_, dst_) 붙여서 구분\n",
        "## => 결과적으로 거래별로 출금+입금 계좌의 그래프 피처가 모두 포함된 테이블 생성\n",
        "\n",
        "# graph_features: node 단위의 그래프 피처 DataFrame (node 컬럼 포함)\n",
        "# df: 거래 단위 원본 데이터프레임\n",
        "\n",
        "# 출금계좌 기준 피처\n",
        "hf_graph_df_with_src = hf_graph_df.merge(\n",
        "    graph_features.add_prefix(\"src_\").rename(columns={\"src_node\": \"wd_account\"}),\n",
        "    on=\"wd_account\",\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "# 입금계좌 기준 피처\n",
        "hf_graph_features_merged = hf_graph_df_with_src.merge(\n",
        "    graph_features.add_prefix(\"dst_\").rename(columns={\"dst_node\": \"dps_account\"}),\n",
        "    on=\"dps_account\",\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "hf_graph_features_merged.head()"
      ],
      "metadata": {
        "id": "NABDCWA_XIj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find all boolean columns\n",
        "bool_cols = hf_graph_features_merged.select_dtypes(include='bool').columns.tolist()\n",
        "print(bool_cols)\n",
        "\n",
        "# Convert boolean columns to integers\n",
        "hf_graph_features_merged['src_round_trip'] = hf_graph_features_merged['src_round_trip'].astype(int)\n",
        "hf_graph_features_merged['src_simple_cycle'] = hf_graph_features_merged['src_simple_cycle'].astype(int)\n",
        "hf_graph_features_merged['dst_round_trip'] = hf_graph_features_merged['dst_round_trip'].astype(int)\n",
        "hf_graph_features_merged['dst_simple_cycle'] = hf_graph_features_merged['dst_simple_cycle'].astype(int)\n",
        "\n",
        "# Ensure account IDs are integers\n",
        "hf_graph_features_merged['wd_account'] = hf_graph_features_merged['wd_account'].astype('int64')\n",
        "hf_graph_features_merged['dps_account'] = hf_graph_features_merged['dps_account'].astype('int64')\n",
        "\n",
        "# Display the final DataFrame summary\n",
        "hf_graph_features_merged.info()"
      ],
      "metadata": {
        "id": "pV8mbyi2XcQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 기초 통계량 피쳐 + 그래프 피쳐 병합"
      ],
      "metadata": {
        "id": "srYleuR1X4Yq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(hf_basic_stats_df.shape)\n",
        "print(hf_graph_features_merged.shape)\n",
        "print(hf_graph_features_merged.columns)"
      ],
      "metadata": {
        "id": "UWOp0M6LYCfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_basic_stats_df.columns"
      ],
      "metadata": {
        "id": "U07hLCpPYfLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_graph_features_merged.columns"
      ],
      "metadata": {
        "id": "ueSkTjNoYyT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 거래 단위로 병합\n",
        "# 거래 단위(row 단위)로 병합하려면 공통 컬럼을 키로 변환하면 됨\n",
        "# 중복행 제거: 만약 같은 거래 단위('datetime', 'wd_account', 'dps_account', 'tran_amt', 'md_type', 'ff_sp_ai')가 두 데이터프레임에 여러번 존재하면\n",
        "# merge시 행이 불어나므로, 병합전 unique처리가 필요함\n",
        "hf_basic_stats_df = hf_basic_stats_df.drop_duplicates(\n",
        "    subset=['datetime', 'wd_account', 'dps_account', 'tran_amt', 'md_type', 'ff_sp_ai']\n",
        ")\n",
        "\n",
        "hf_graph_df = hf_graph_df.drop_duplicates(\n",
        "    subset=['datetime', 'wd_account', 'dps_account', 'tran_amt', 'md_type', 'ff_sp_ai']\n",
        ")\n",
        "\n",
        "\n",
        "hf_merged = pd.merge(hf_basic_stats_df, hf_graph_features_merged,\n",
        "    on=['datetime', 'wd_account', 'dps_account', 'tran_amt', 'md_type', 'fnd_type', 'ff_sp_ai'], # 공통 컬럼을 키로 병합\n",
        "    how='left',\n",
        "    suffixes=('_basic_stats', '_graph')) # 혹시 겹치는 컬럼이 있으면 구분\n",
        "\n",
        "hf_merged.head()"
      ],
      "metadata": {
        "id": "cdgQrxiqZErk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_merged.shape"
      ],
      "metadata": {
        "id": "aSJujt8yZUTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 전처리"
      ],
      "metadata": {
        "id": "o0zD5xrkZcQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop account identifier columns\n",
        "hf_merged.drop(columns=['wd_account', 'dps_account'], axis=1, inplace=True)\n",
        "\n",
        "# '# 결측치 확인' (Check for missing values)\n",
        "print(hf_merged.isnull().sum())\n",
        "print('----------------------------------------------------')\n",
        "print(hf_merged.isnull().mean())\n",
        "print('----------------------------------------------------')\n",
        "\n",
        "missing_info = hf_merged.isnull().sum()\n",
        "missing_info = missing_info[missing_info > 0]\n",
        "print(missing_info)"
      ],
      "metadata": {
        "id": "iIJ3YibdZhJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 결측치 처리 및 확인\n",
        "hf_merged['ff_sp_ai'] = hf_merged['ff_sp_ai'].fillna('00')\n",
        "print(hf_merged.isnull().mean())"
      ],
      "metadata": {
        "id": "bZghZK3nZ3Ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ff_sp_ai 변수 분포 확인\n",
        "hf_merged['ff_sp_ai'].value_counts()"
      ],
      "metadata": {
        "id": "v80EOi6UaDHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 라벨 인코딩 (Label Encoding)\n",
        "le = LabelEncoder()\n",
        "\n",
        "hf_merged.loc[:, 'ff_sp_ai_encoded'] = le.fit_transform(hf_merged['ff_sp_ai'])\n",
        "\n",
        "# 기존 컬럼 삭제 및 인코딩된 결과 확인\n",
        "hf_merged.drop(columns=['ff_sp_ai'], inplace=True)\n",
        "print(hf_merged['ff_sp_ai_encoded'].value_counts())"
      ],
      "metadata": {
        "id": "aY0x2GD_aM0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ff_sp_ai_encoded 값을 target 0 또는 1로 변경\n",
        "# 01, 02, SP -> target 1, 이외 target 0 으로 설정\n",
        "hf_merged['target'] = 0\n",
        "hf_merged.loc[hf_merged['ff_sp_ai_encoded'] == 1, 'target'] = 1\n",
        "hf_merged.loc[hf_merged['ff_sp_ai_encoded'] == 2, 'target'] = 1\n",
        "hf_merged.loc[hf_merged['ff_sp_ai_encoded'] == 3, 'target'] = 1\n",
        "\n",
        "hf_merged.head()"
      ],
      "metadata": {
        "id": "z0XwtxWMaxs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 날짜별 오름차순 정렬\n",
        "hf_merged.sort_values(by='datetime', ascending=True, inplace=True)\n",
        "\n",
        "hf_merged.head()"
      ],
      "metadata": {
        "id": "eUfITrzpa_4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ff_sp_ai_encoded 제거\n",
        "hf_merged.drop(['ff_sp_ai_encoded'], axis=1, inplace=True)\n",
        "\n",
        "hf_merged.head()"
      ],
      "metadata": {
        "id": "3ASwd6YNbLpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train/valid/test set 분할\n",
        "\n",
        "# train set: 2024-01-01 ~ 2024-07-31\n",
        "# valid set: 2024-08-01 ~ 2024-10-31\n",
        "# test set: 2024-11-01 ~ 2024-12-31\n",
        "\n",
        "train_end = '2024-07-31 23:59:59'\n",
        "valid_end = '2024-10-31 23:59:59'\n",
        "\n",
        "train_2 = hf_merged[hf_merged['datetime'] <= train_end]\n",
        "valid_2 = hf_merged[(hf_merged['datetime'] > train_end) & (hf_merged['datetime'] <= valid_end)]\n",
        "test_2 = hf_merged[hf_merged['datetime'] > valid_end]\n",
        "\n",
        "\n",
        "print(f\"학습셋 shape: {train_2.shape}, 정상건수: {train_2[train_2['target'] == 0].shape}, 부정건수: {train_2[train_2['target'] == 1].shape}\")\n",
        "train_fraud_ratio = train_2[train_2['target'] == 1].shape[0] / train_2.shape[0] * 100\n",
        "print(f\"학습셋 fraud 비율: {train_fraud_ratio:.3f}%\")\n",
        "\n",
        "print('----------------------------------------------------')\n",
        "\n",
        "print(f\"검증셋 shape: {valid_2.shape}, 정상건수: {valid_2[valid_2['target'] == 0].shape}, 부정건수: {valid_2[valid_2['target'] == 1].shape}\")\n",
        "valid_fraud_ratio = valid_2[valid_2['target'] == 1].shape[0] / valid_2.shape[0] * 100\n",
        "print(f\"검증셋 fraud 비율: {valid_fraud_ratio:.3f}%\")\n",
        "\n",
        "print('----------------------------------------------------')\n",
        "\n",
        "print(f\"테스트셋 shape: {test_2.shape}, 정상건수: {test_2[test_2['target'] == 0].shape}, 부정건수: {test_2[test_2['target'] == 1].shape}\")\n",
        "test_fraud_ratio = test_2[test_2['target'] == 1].shape[0] / test_2.shape[0] * 100\n",
        "print(f\"테스트셋 fraud 비율: {test_fraud_ratio:.3f}%\")"
      ],
      "metadata": {
        "id": "NNrh8sEjbfEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_2.columns)"
      ],
      "metadata": {
        "id": "qSZFoV-Pbsm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_2.shape)"
      ],
      "metadata": {
        "id": "VtjJHi_2bupt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate features (X) and target (y) for each set\n",
        "X_train = train_2.iloc[:, :-1]\n",
        "y_train = train_2.iloc[:, -1]\n",
        "\n",
        "X_valid = valid_2.iloc[:, :-1]\n",
        "y_valid = valid_2.iloc[:, -1]\n",
        "\n",
        "X_test = test_2.iloc[:, :-1]\n",
        "y_test = test_2.iloc[:, -1]\n",
        "\n",
        "# Print the shapes to verify (Note: a typo in the original print is corrected below)\n",
        "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
        "print(f\"X_valid: {X_valid.shape}, y_valid: {y_valid.shape}\")\n",
        "print(f\"X_test:  {X_test.shape},  y_test:  {y_test.shape}\")\n",
        "\n",
        "\n",
        "# Convert datetime to a numeric (integer) format for the model\n",
        "X_train['datetime'] = X_train['datetime'].dt.strftime('%Y%m%d%H%M%S').astype('int64')\n",
        "X_valid['datetime'] = X_valid['datetime'].dt.strftime('%Y%m%d%H%M%S').astype('int64')\n",
        "X_test['datetime'] = X_test['datetime'].dt.strftime('%Y%m%d%H%M%S').astype('int64')"
      ],
      "metadata": {
        "id": "Tb9yvlJVcC1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.drop(['datetime'], axis=1, inplace=True)\n",
        "X_valid.drop(['datetime'], axis=1, inplace=True)\n",
        "X_test.drop(['datetime'], axis=1, inplace=True)\n",
        "\n",
        "X_train.head()"
      ],
      "metadata": {
        "id": "w09onZBicQbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# datetime_int 를 컬럼의 맨앞으로 위치 변경\n",
        "# 마지막 컬럼의 이름을 가져오기\n",
        "last_col = X_train.columns[-1]\n",
        "\n",
        "# pop으로 마지막 컬럼을 제거하고, insert로 맨 앞에 추가.\n",
        "X_train.insert(0, last_col, X_train.pop(last_col))\n",
        "X_valid.insert(0, last_col, X_valid.pop(last_col))\n",
        "X_test.insert(0, last_col, X_test.pop(last_col))\n",
        "\n",
        "# 타겟 클래스 불균형 처리\n",
        "sm = SMOTE(sampling_strategy='minority', random_state=42)\n",
        "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
        "\n",
        "# 데이터 스케일링\n",
        "scaler = MinMaxScaler()\n",
        "X_train_res_scaled = scaler.fit_transform(X_train_res)\n",
        "X_valid_scaled = scaler.transform(X_valid)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "XWERS8hFcaXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 학습"
      ],
      "metadata": {
        "id": "bX3rqqdAdkC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습 - Logistic Regression\n",
        "LR_clf = LogisticRegression(random_state=42)\n",
        "LR_clf.fit(X_train_res_scaled, y_train_res)\n",
        "print('--------------------LR_clf fit completed--------------------')\n",
        "\n",
        "# 모델 학습 - Decision Tree\n",
        "DT_clf = DecisionTreeClassifier(random_state=42)\n",
        "DT_clf.fit(X_train_res_scaled, y_train_res)\n",
        "print('--------------------DT_clf fit completed--------------------')\n",
        "\n",
        "# RandomForestClassifier (주석 처리됨)\n",
        "# RF_clf = RandomForestClassifier(n_estimators=100, max_depth=None, random_state=42)\n",
        "# RF_clf.fit(X_train_res_scaled, y_train_res)\n",
        "# print('--------------------RF_clf fit completed--------------------')\n",
        "\n",
        "# XGBClassifier\n",
        "XGB_clf = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, subsample=0.8, colsample_bytree=0.8, use_label_encoder=False, eval_metric='aucpr', random_state=42)\n",
        "XGB_clf.fit(X_train_res_scaled, y_train_res)\n",
        "print('--------------------XGB_clf fit completed--------------------')\n",
        "\n",
        "# LGBMClassifier\n",
        "LGBM_clf = LGBMClassifier(n_estimators=100, learning_rate=0.1, max_depth=-1, random_state=42)\n",
        "LGBM_clf.fit(X_train_res_scaled, y_train_res)\n",
        "print('--------------------LGBM_clf fit completed--------------------')\n",
        "\n",
        "# CatBoostClassifier\n",
        "CB_clf = CatBoostClassifier(iterations=200, learning_rate=0.1, depth=6, loss_function='Logloss', eval_metric='AUC', class_weights=[1, 10], random_state=42, verbose=1)\n",
        "CB_clf.fit(X_train_res_scaled, y_train_res, eval_set=[(X_test_scaled, y_test)], verbose=50)\n",
        "print('--------------------CB_clf fit completed--------------------')\n"
      ],
      "metadata": {
        "id": "49InQw0IdB2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 검증세트 성능 평가"
      ],
      "metadata": {
        "id": "dJRfyV_Hdost"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================ 검증셋 성능 평가 ================\n",
        "\n",
        "# Logistic Regression Classifier 성능지표\n",
        "LR_y_valid_pred = LR_clf.predict(X_valid_scaled)\n",
        "print(f\"LR valid roc_auc_score:  {roc_auc_score(y_valid, LR_y_valid_proba):.3f}\") # 확률값 사용\n",
        "print(f\"LR valid accuracy_score: {accuracy_score(y_valid, LR_y_valid_pred):.3f}\")\n",
        "print(f\"LR valid precision_score:{precision_score(y_valid, LR_y_valid_pred):.3f}\")\n",
        "print(f\"LR valid recall_score:   {recall_score(y_valid, LR_y_valid_pred):.3f}\")\n",
        "print(f\"LR valid f1_score:       {f1_score(y_valid, LR_y_valid_pred):.3f}\")\n",
        "\n",
        "\n",
        "# DecisionTree Classifier 성능지표\n",
        "DT_y_valid_pred = DT_clf.predict(X_valid_scaled)\n",
        "print(f\"DT valid roc_auc_score:  {roc_auc_score(y_valid, DT_y_valid_proba):.3f}\") # 확률값 사용\n",
        "print(f\"DT valid accuracy_score: {accuracy_score(y_valid, DT_y_valid_pred):.3f}\")\n",
        "print(f\"DT valid precision_score:{precision_score(y_valid, DT_y_valid_pred):.3f}\")\n",
        "print(f\"DT valid recall_score:   {recall_score(y_valid, DT_y_valid_pred):.3f}\")\n",
        "print(f\"DT valid f1_score:       {f1_score(y_valid, DT_y_valid_pred):.3f}\")\n",
        "\n",
        "\n",
        "# (RandomForest Classifier 성능지표 - 주석 처리됨)\n",
        "# RF_y_valid_pred = RF_clf.predict(X_valid_scaled)\n",
        "# print(f\"RF valid roc_auc_score:  {roc_auc_score(y_valid, RF_y_valid_proba):.3f}\")\n",
        "# print(f\"RF valid accuracy_score: {accuracy_score(y_valid, RF_y_valid_pred):.3f}\")\n",
        "# print(f\"RF valid precision_score:{precision_score(y_valid, RF_y_valid_pred):.3f}\")\n",
        "# print(f\"RF valid recall_score:   {recall_score(y_valid, RF_y_valid_pred):.3f}\")\n",
        "# print(f\"RF valid f1_score:       {f1_score(y_valid, RF_y_valid_pred):.3f}\")\n",
        "\n",
        "\n",
        "# XGBoost Classifier 성능지표\n",
        "XGB_y_valid_pred = XGB_clf.predict(X_valid_scaled)\n",
        "print(f\"XGB valid roc_auc_score:  {roc_auc_score(y_valid, XGB_y_valid_proba):.3f}\")\n",
        "print(f\"XGB valid accuracy_score: {accuracy_score(y_valid, XGB_y_valid_pred):.3f}\")\n",
        "print(f\"XGB valid precision_score:{precision_score(y_valid, XGB_y_valid_pred):.3f}\")\n",
        "print(f\"XGB valid recall_score:   {recall_score(y_valid, XGB_y_valid_pred):.3f}\")\n",
        "print(f\"XGB valid f1_score:       {f1_score(y_valid, XGB_y_valid_pred):.3f}\")\n",
        "\n",
        "\n",
        "# LightGBM Classifier 성능지표\n",
        "LGBM_y_valid_pred = LGBM_clf.predict(X_valid_scaled)\n",
        "print(f\"LGBM valid roc_auc_score:  {roc_auc_score(y_valid, LGBM_y_valid_proba):.3f}\")\n",
        "print(f\"LGBM valid accuracy_score: {accuracy_score(y_valid, LGBM_y_valid_pred):.3f}\")\n",
        "print(f\"LGBM valid precision_score:{precision_score(y_valid, LGBM_y_valid_pred):.3f}\")\n",
        "print(f\"LGBM valid recall_score:   {recall_score(y_valid, LGBM_y_valid_pred):.3f}\")\n",
        "print(f\"LGBM valid f1_score:       {f1_score(y_valid, LGBM_y_valid_pred):.3f}\")\n",
        "\n",
        "\n",
        "# CatBoost Classifier 성능지표\n",
        "CB_y_valid_pred = CB_clf.predict(X_valid_scaled)\n",
        "print(f\"CB valid roc_auc_score:  {roc_auc_score(y_valid, CB_y_valid_proba):.3f}\")\n",
        "print(f\"CB valid accuracy_score: {accuracy_score(y_valid, CB_y_valid_pred):.3f}\")\n",
        "print(f\"CB valid precision_score:{precision_score(y_valid, CB_y_valid_pred):.3f}\")\n",
        "print(f\"CB valid recall_score:   {recall_score(y_valid, CB_y_valid_pred):.3f}\")\n",
        "print(f\"CB valid f1_score:       {f1_score(y_valid, CB_y_valid_pred):.3f}\")"
      ],
      "metadata": {
        "id": "00Ic1BWidq6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Set 성능 평가"
      ],
      "metadata": {
        "id": "bClyJi2mf7kV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Logistic Regression 모델 학습의 경우\n",
        "# LR_y_test_prob = LR_clf.predict_proba(X_test_scaled)[:,1]\n",
        "# # y_test_true = y_test.values\n",
        "\n",
        "# # X_test['y_test_prob_score'] = LR_y_test_prob\n",
        "# # X_test['y_test_true'] = y_test_true\n",
        "\n",
        "# # test_pred_df = X_test[['datetime_int', 'y_test_prob_score', 'y_true']]\n",
        "\n",
        "# # DecisionTree 모델 학습의 경우\n",
        "# DT_y_test_prob = DT_clf.predict_proba(X_test_scaled)[:,1]\n",
        "# # y_test_true = y_test.values\n",
        "\n",
        "# # X_test['y_test_prob_score'] = DT_y_test_prob\n",
        "# # X_test['y_test_true'] = y_test_true\n",
        "\n",
        "# # test_pred_df = X_test[['datetime_int', 'y_test_prob_score', 'y_true']]\n",
        "\n",
        "# # # RandomForest 모델 학습의 경우\n",
        "# # RF_y_test_prob = RF_clf.predict_proba(X_test_scaled)[:,1]\n",
        "# # y_test_true = y_test.values\n",
        "\n",
        "# # X_test['y_test_prob_score'] = RF_y_test_prob\n",
        "# # X_test['y_test_true'] = y_test_true\n",
        "\n",
        "# # test_pred_df = X_test[['datetime_int', 'y_test_prob_score', 'y_true']]"
      ],
      "metadata": {
        "id": "87ud-kY7egEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # XGBoost 모델 학습의 경우\n",
        "# XGB_y_test_prob = XGB_clf.predict_proba(X_test_scaled)[:,1]\n",
        "# # y_test_true = y_test.values\n",
        "\n",
        "# # X_test['y_test_prob_score'] = XGB_y_test_prob\n",
        "# # X_test['y_true'] = y_test_true\n",
        "\n",
        "# # test_pred_df = X_test[['datetime_int', 'y_test_prob_score', 'y_true']]\n",
        "\n",
        "\n",
        "# # # LightGBM 모델 학습의 경우\n",
        "# # LGBM_y_test_prob = LGBM_clf.predict_proba(X_test_scaled)[:,1]\n",
        "# # y_test_true = y_test.values\n",
        "\n",
        "# # X_test['y_test_prob_score'] = LGBM_y_test_prob\n",
        "# # X_test['y_true'] = y_test_true\n",
        "\n",
        "# # test_pred_df = X_test[['datetime_int', 'y_test_prob_score', 'y_true']]\n",
        "\n",
        "\n",
        "# # CatBoost 모델 학습의 경우\n",
        "CB_y_test_prob = CB_clf.predict_proba(X_test_scaled)[:,1]\n",
        "y_test_true = y_test.values\n",
        "\n",
        "X_test['y_test_prob_score'] = CB_y_test_prob\n",
        "X_test['y_true'] = y_test_true\n",
        "\n",
        "test_pred_df = X_test[['datetime_int', 'y_test_prob_score', 'y_true']]"
      ],
      "metadata": {
        "id": "MVJEjKYrghrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top-K@일 평가\n",
        "# 매일 상위 K건만 심사 가능(예: 10건) -> 31일이면 예측 결과 총 310건을 하나로 합쳐서 Precision/Recall/F1 계산\n",
        "K = 10\n",
        "\n",
        "# 날짜별로 점수 상위 k\n",
        "topk_by_day = (\n",
        "    test_pred_df.sort_values(['datetime_int', 'y_test_prob_score'], ascending=[True, False])\n",
        "    .groupby('datetime_int', group_keys=False)\n",
        "    .head(K)\n",
        ")\n",
        "\n",
        "tp = topk_by_day['y_true'].sum()\n",
        "pred_total = len(topk_by_day)          # 31 * K (일수가 31일일 때)\n",
        "pos_total = test_pred_df['y_true'].sum()\n",
        "\n",
        "precision_atK = (tp / pred_total) if pred_total else 0\n",
        "recall_atK = (tp / pos_total) if pos_total else 0\n",
        "f1_atK = (2*precision_atK*recall_atK/(precision_atK+recall_atK)) if (precision_atK+recall_atK) > 0 else 0\n",
        "\n",
        "print(f\"Precision@{K*31}: {precision_atK:.3f}, Recall@{K*31}: {recall_atK:.3f}, F1@{K*31}: {f1_atK:.3f}\")"
      ],
      "metadata": {
        "id": "JnrYGiKpgw5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 스코어 카드 계산\n",
        "\n",
        "# y_test_prob_score * 1000 스케일링\n",
        "test_pred_df['y_test_prob_score_scaled'] = (test_pred_df['y_test_prob_score']*1000).astype(int)\n",
        "\n",
        "# 내림차순 정렬 (스코어 점수 높은 순서 먼저)\n",
        "test_pred_df = test_pred_df.sort_values('y_test_prob_score_scaled', ascending=False).reset_index(drop=True)\n",
        "\n",
        "# 구간(binning) 10점 단위\n",
        "bins = list(range(0, 1001, 10))\n",
        "labels = [f'[{b}-{b+10})' for b in bins[:-1]]\n",
        "test_pred_df['bin'] = pd.cut(test_pred_df['y_test_prob_score_scaled'], bins=bins, right=False, labels=labels, include_lowest=True)\n",
        "\n",
        "# 구간별 정상/부정 카운트\n",
        "grp = test_pred_df.groupby('bin', observed=True).agg(\n",
        "    normal_cnt = ('y_true', lambda x: (x==0).sum()),\n",
        "    fraud_cnt = ('y_true', lambda x: (x==1).sum()),\n",
        "    count = ('y_true', 'size'),\n",
        "    max_y_test_prob_score = ('y_test_prob_score_scaled', 'max')\n",
        ").reset_index()\n",
        "\n",
        "# bin은 높은 점수부터 누적해야 하므로 max_y_test_prob_score 기준으로 내림차순 정렬\n",
        "grp = grp.sort_values('max_y_test_prob_score', ascending=False).reset_index(drop=True)\n",
        "\n",
        "# 누적 계산\n",
        "grp['cum_fraud'] = grp['fraud_cnt'].cumsum() # A\n",
        "grp['cum_total'] = grp['count'].cumsum()    # B\n",
        "total_fraud = test_pred_df['y_true'].sum()"
      ],
      "metadata": {
        "id": "L_gGoG8DhK94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate cumulative precision percentage\n",
        "grp['cum_precision_pct'] = (grp['cum_fraud'] / grp['cum_total']*100).round(3)\n",
        "# Calculate cumulative recall (capture rate) percentage\n",
        "grp['cum_recall_pct'] = (grp['cum_fraud'] / total_fraud *100).round(3)\n",
        "\n",
        "# For F1-score: calculate P and R as ratios first, then convert to percentage\n",
        "P = grp[\"cum_fraud\"] / grp[\"cum_total\"]\n",
        "R = grp[\"cum_fraud\"] / total_fraud\n",
        "# Calculate F1-score, handling division by zero, then convert to pct and fill NaNs\n",
        "grp[\"cum_f1_pct\"] = (2 * P * R / (P + R).replace(0, np.nan) * 100).fillna(0).round(3)\n",
        "\n",
        "# Select only the most readable columns for the final report\n",
        "report = grp[[\n",
        "    \"bin\", \"normal_cnt\", \"fraud_cnt\",\n",
        "    \"cum_fraud\", \"cum_total\",\n",
        "    \"cum_precision_pct\", \"cum_recall_pct\", \"cum_f1_pct\"\n",
        "]]"
      ],
      "metadata": {
        "id": "mlT7Ht5Ahbs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the final report\n",
        "report"
      ],
      "metadata": {
        "id": "N3WYJ-AOhhhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Importance"
      ],
      "metadata": {
        "id": "9IL_oTCBh4mz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Importance 계산\n",
        "from catboost import Pool\n",
        "\n",
        "# CatBoost Pool 객체를 사용하여 학습 데이터에 대한 중요도를 계산.\n",
        "importances = CB_clf.get_feature_importance(Pool(X_train_res_scaled, y_train_res))\n",
        "\n",
        "# 결과를 DataFrame으로 만들고 상위 10개를 확인.\n",
        "feature_importance = pd.DataFrame({\n",
        "    \"feature\": X_train.columns,\n",
        "    \"importance\": importances\n",
        "}).sort_values(by=\"importance\", ascending=False).head(10)\n",
        "\n",
        "print(feature_importance)"
      ],
      "metadata": {
        "id": "1HgfuzSeh7h0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "\n",
        "# 한글 폰트 설정 (윈도우: 맑은 고딕)\n",
        "rc('font', family='Malgun Gothic')\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "# 수평 막대 그래프 생성\n",
        "bars = plt.barh(feature_importance['feature'], feature_importance['importance'], height=0.5, color='lightcoral')\n",
        "\n",
        "# 막대 끝에 소수점 셋째 자리까지 값 표시\n",
        "plt.bar_label(bars, fmt='%.3f', label_type=\"edge\", padding=1)\n",
        "\n",
        "# X축 레이블 및 제목 설정\n",
        "plt.xlabel(\"Top 10 피처 중요도 by CatBoost\")\n",
        "plt.title(\"피처 중요도 by CatBoost\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wCPUXcjbiV6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DataFrame으로 보기\n",
        "df_importance = pd.DataFrame({\n",
        "    'feature': X_train.columns,\n",
        "    'importance': LGBM_clf.feature_importances_\n",
        "}).sort_values(by='importance', ascending=False)\n",
        "\n",
        "# print(df_importance)\n",
        "\n",
        "# 시각화\n",
        "top10 = df_importance.head(10)\n",
        "bars = plt.barh(top10['feature'], top10['importance'], height=0.5, color='lightcoral')\n",
        "plt.bar_label(bars, fmt=\"%.f\", label_type=\"edge\", padding=1)\n",
        "plt.xlabel(\"Top 10 피처 중요도 by LightGBM\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QpTx8IariiaG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}